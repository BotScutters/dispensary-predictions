{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling Mary Jane\n",
    "==============\n",
    "\n",
    "***Using machine earning to reveal insights and predict performance of cannabis dispensaries***\n",
    "\n",
    "**Author:** *Scott Butters*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "In 2012, Washington state passed I-502 and legalized the recreational sale, use, and possession of marijuana. This event has led to an explosion of development in the field that's making waves through our society. Since 2014, approximately 500 state licensed dispensaries have opened throughout the state, with nearly 150 of those here in Seattle. And the thing that gets me excited is that because of regulations, a ton of their data is publicly available, meaning I got to play with it. We can go into the weeds later about how and why it took me a thousand websites and 4 different scraping techniques to get the data, and how my R^2 varied when I went shifted from 10-30 features to estimate the monthly revenue of a cannabis dispensary. But for now, let me tell you about what I learned as I immersed myself in this dataset over the past 2 weeks. In this project I scour the web for publicly available data that might be predictive of how a cannabis dispensary performs, such as customer reviews, inventory distributions, and local demographics. I then train machine learning models to predict a dispensary's monthly revenue and analyze the resulting models to distill insights about what drives sales in the marijuana market.\n",
    "\n",
    "\n",
    "In this project, I explored the question of what makes the Washington recreational cannabis market tick. I decided that if I could identify the features that most contributed to a dispensary's revenue, I could get a lot of insight out of how the features shake out. \n",
    "\n",
    "When asked what my project was, I kept saying that it was to use web scraping to predict monthly revenue using linear regression. And it was, of course it was. But that's not the point, that's not the story. \n",
    "\n",
    "Here's the monthly revenue of all dispensaries for the last 16 months. See the trend?\n",
    "\n",
    "Okay. Now. Here's the monthly revenue of a handful of dispensaries over that same period. See the trend?\n",
    "\n",
    "But we can show this better. Here's the average monthly change in revenue of a dispensary.\n",
    "Next to the change for the overall market. \n",
    "Next to variation in the S&P 500 over the same period.\n",
    "And now the top marijuana ETF out there.\n",
    "\n",
    "So if we're going to make choices about what to invest in to make a dispensary successful, we need to have the impacts of the external market in context. So I put those indicators in there for some regression models to see how significant a role they played.\n",
    "\n",
    "In at attempt to both have more data and isolate the time component of my analysis, I constructed a new target variable, which is marketshare. For each month, I divided the revenue of a given dispensary by the revenue of all of the dispensaries over that period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain the Data\n",
    "\n",
    "The data for this project is derived from several sources:\n",
    "\n",
    "## Dispensary profiles from [Leafly](www.leafly.com)\n",
    "\n",
    "Leafly is an information aggregator for cannabis. They maintain a profile for most of the dispensaries in the state. As part of my dataset, I've scraped the following features from the Leafly website for each dispensary for which it was available:\n",
    "\n",
    "* Average customer rating and number of customer reviews\n",
    "* Inventory counts (number of products under descriptions like \"flower\", \"edibles\", \"concentrates\", etc.\n",
    "* Categorical qualities, such as whether or not the store is ADA accessible or has an ATM onsite\n",
    "* Metadata such as name, address, phone number, etc.\n",
    "\n",
    "The combination of these features gives us a profile of each dispensary that allow us to draw insights from our model into what makes for a successful dispensary.\n",
    "\n",
    "## Demographics from [WA HomeTownLocator](https://washington.hometownlocator.com/)\n",
    "\n",
    "Of course, having the best inventory, friendliest staff and prettiest building in the state doesn't amount to anything if a dispensary is in the middle of nowhere. This is where demographic data comes in. WA HomeTownLocator maintains a database of demographic statistics for nearly every zip code in the state of Washington. The data is produced by Esri Demographics, and updated 4 times per year using data from the federal census, IRS, USPS, as well as local data sources and more. From this website I scraped data likely to be predictive of a local market such as:\n",
    "\n",
    "* Population density\n",
    "* Diversity\n",
    "* Average income\n",
    "\n",
    "These data give our model an image of what a dispensary's customer base is like, allowing us to characterize what makes for a good location to establish a dispensary.\n",
    "\n",
    "## [Washington State Liquor and Cannabis Board (WSLCB)](https://lcb.wa.gov/)\n",
    "\n",
    "Lastly, all that data would get us nowhere if we didn't have any target data to train our models on. That's where the WSLCB comes in. The WSLCB maintains data on every dispensary in the state, including monthly reports of revenue (which is what our model is predicting). Their data is scattered across a couple of different outlets, but for this project I used spreadsheets downloadable from [this obsure page](https://lcb.wa.gov/records/frequently-requested-lists) to get sales data dating back to November 2017. Because the only identifying information in that spreadsheet is the license number of the dispensary, I also downloaded a spreadsheet listing metadata for every entity that has applied for a Marijuana license, which I then joined with the sales data in order to link it up with data scraped from other resources.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "The code below contains a pipeline to visit each of our sources and scrape or download all of the desired data into a few files stored in the data/raw/ directory to be scrubbed and processed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/data/make_dataset.py\n",
    "\n",
    "# Imports\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Helper functions\n",
    "def parse_products(text):\n",
    "    '''\n",
    "    Parses string of products into dictionary of products with counts\n",
    "    Input: string of products as scraped from Leafly dispensary page\n",
    "    Output: dictionary of {product: count} relationships\n",
    "    '''\n",
    "    repl = ['(', ')']\n",
    "    for char in repl:\n",
    "        text = text.replace(char, '')\n",
    "    prod_list = text.split('\\n')\n",
    "    prod_list = [prod.strip().lower() for prod in prod_list]\n",
    "    prod_dict = {}\n",
    "    for i, element in enumerate(prod_list):\n",
    "        if element.isnumeric():\n",
    "            prod_dict[prod_list[i - 1]] = int(element)\n",
    "        elif 'difference' in element:\n",
    "            pass\n",
    "        else:\n",
    "            prod_dict[element.strip()] = 0\n",
    "    return prod_dict\n",
    "\n",
    "\n",
    "def scrape_disp(disp, driver, user_agent):\n",
    "    \"\"\"\n",
    "    Scrapes dispensary-specific page on leafly for additional data and adds it\n",
    "    to existing dictionary dataset\n",
    "    Input: dictionary containing metadata for a single dispensary\n",
    "    Output: dictionary with additional metadata for given dispensary\n",
    "    \"\"\"\n",
    "    url = 'https://www.leafly.com/dispensary-info/'\n",
    "    slug = disp['slug']\n",
    "    url += slug\n",
    "    \n",
    "    if 'OR' in disp['formattedShortLocation']:\n",
    "        return {}\n",
    "    \n",
    "    response  = requests.get(url, headers=user_agent)\n",
    "    if not response.ok:\n",
    "        print('Connection to {} failed'.format(disp['name']))\n",
    "        return {}\n",
    "    \n",
    "    # Open page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Confirm over 21\n",
    "    try:\n",
    "        yes_button = driver.find_element_by_xpath('//button[text()=\"Yes\"]')\n",
    "        yes_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Scrape categoricals\n",
    "    try:\n",
    "        cat_selector = driver.find_element_by_class_name('jsx-4153842418')\n",
    "        # cat_selector = driver.find_element_by_tag_name('ul')\n",
    "        items = cat_selector.find_elements_by_tag_name(\"li\")\n",
    "        categories = {item.text.lower(): True for item in items}\n",
    "        disp.update(categories)\n",
    "    except:\n",
    "        print('Failed to scrape categories for {}'.format(disp['name']))\n",
    "        pass\n",
    "\n",
    "    # Scrape products\n",
    "    try:\n",
    "        products = driver.find_elements_by_class_name('jsx-1433915045')\n",
    "        products_text = products[0].text\n",
    "        product_dict = parse_products(products_text)\n",
    "        disp.update(product_dict)\n",
    "    except:\n",
    "        print('Failed to scrape products for {}'.format(disp['name']))\n",
    "        pass\n",
    "    \n",
    "    print('Successfully scraped {}'.format(disp['name']))\n",
    "    return disp\n",
    "\n",
    "\n",
    "def scrape_leafly_disps(path, disp_data_filename, data):\n",
    "    \"\"\"\n",
    "    Gets JSON file of data on dispensaries from Leafly, either by loading\n",
    "    pre-existing file or by re-scraping Leafly\n",
    "    Input: path and filename for output file, index  of basic dispensary metadata\n",
    "    Output: Index formatted JSON with one dictionary for each found dispensary\n",
    "    \"\"\"\n",
    "#     filepath = '../data/raw/dispensary_data.json'\n",
    "    filepath = path + disp_data_filename\n",
    "    if os.path.isfile(filepath):\n",
    "        overwrite = input(\n",
    "            '''Dispensaries data dict already exists. Scrape data again? y/n\\n\n",
    "            Note: this could take several minutes.''')\n",
    "        if overwrite.lower() != 'y':\n",
    "            with open(filepath) as json_file:\n",
    "                data = json.load(json_file)\n",
    "            return data\n",
    "\n",
    "    print(\"Beginning scrape...\")\n",
    "    ua = UserAgent()\n",
    "    user_agent = {'User-agent': ua.random}\n",
    "    chromedriver = \"/Applications/chromedriver\"\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "    for disp in data:\n",
    "        new_data = scrape_disp(data[disp], driver, user_agent)\n",
    "        data[disp].update(new_data)\n",
    "    \n",
    "    with open(filepath, 'w') as outfile:  \n",
    "        json.dump(data, outfile)\n",
    "        print('Scraped data written to {}'.format(filepath))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def retry(TL_lat, TL_lon, cell_size):\n",
    "    '''\n",
    "    If request hits Leafly API limit, split cell into 4 subcells and retry\n",
    "    Input: Lat/lon coordinates for top left of map and optionally a size for\n",
    "    the map area (defaults to 0.5)\n",
    "    Output: dictionary of dictionaries containing metadata for each dispensary \n",
    "    found in map area\n",
    "    '''\n",
    "    TL_lats = [TL_lat, TL_lat - 0.4 * cell_size]\n",
    "    TL_lons = [TL_lat, TL_lat + 0.4 * cell_size]\n",
    "    disp_data = {}\n",
    "    for lat, lon in zip(TR_lats, TR_lons):\n",
    "        data = get_disp_data_by_coords(lat, lon, cell_size=0.6 * cell_size)\n",
    "        disp_data.update(data)\n",
    "    return disp_data\n",
    "\n",
    "\n",
    "def get_disp_data_by_coords(TL_lat, TL_lon, cell_size=0.5):\n",
    "    \"\"\"\n",
    "    Performs search for all dispensaries within a map region on Leafly\n",
    "    Input: Lat/lon coordinates for top left of map and optionally a size for\n",
    "    the map area (defaults to 0.5)\n",
    "    Output: dictionary of dictionaries containing metadata for each dispensary \n",
    "    found in map area\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    BR_lat = TL_lat - cell_size\n",
    "    BR_lon = TL_lon + cell_size\n",
    "    coords = TL_lat, TL_lon, BR_lat, BR_lon\n",
    "    \n",
    "    url = (\n",
    "        'https://web-finder.leafly.com/api/searchThisArea?topLeftLat={}&topLeftLon={}&bottomRightLat={}&bottomRightLon={}&userLat=47.6&userLon=-122.3'\n",
    "        ).format(TL_lat, TL_lon, BR_lat, BR_lon)\n",
    "    \n",
    "    # Scrape\n",
    "    time.sleep(.5+2*random.random())\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print('Leafly search failed at {}'.format(coords))\n",
    "        return {}\n",
    "    disps = r.json()\n",
    "    \n",
    "    # Parse\n",
    "    fields = ['name', 'address1', 'address2', 'city', 'location', 'phone',\n",
    "              'formattedShortLocation', 'medical', 'recreational', 'tier', \n",
    "              'lastMenuUpdate', 'starRating', 'numberOfReviews', 'slug']\n",
    "\n",
    "    disp_data = {\n",
    "        d['name']: {k: d[k] for k in fields} for d in disps['dispensaries']}\n",
    "    entries = len(disp_data)\n",
    "    \n",
    "    # Check results; retry if necessary and return data\n",
    "    if entries > 200:\n",
    "        return retry(TR_lat, TR_lon, cell_size)\n",
    "    elif entries < 1:\n",
    "#         print('no results at {}'.format(coords))\n",
    "        return {}\n",
    "    else:\n",
    "#         print('{} results found at {}'.format(len(disp_data), coords))\n",
    "        return disp_data\n",
    "    \n",
    "    \n",
    "def get_rect_disp_data(TL_lat, TL_lon, BR_lat, BR_lon, cell_size=0.5):\n",
    "    \"\"\"\n",
    "    Performs grid search on sub-rectangles with slight overlap, gathering data \n",
    "    on each cell\n",
    "    Input: lat/lon coords of top left and bottom right corners, as well as \n",
    "    optional cell size parameter (defaults to 0.5)\n",
    "    Output: dictionary of dictionaries representing all dispensaries in\n",
    "    rectangle\n",
    "    \"\"\"\n",
    "    coords = TL_lat, TL_lon, BR_lat, BR_lon\n",
    "    max_step = 0.8 * cell_size\n",
    "    lat_steps = np.ceil((TL_lat - BR_lat - cell_size) / max_step)\n",
    "    lon_steps = np.ceil((BR_lon - TL_lon - cell_size) / max_step)\n",
    "\n",
    "    TL_lats = np.linspace(TL_lat, BR_lat + cell_size, lat_steps + 1)\n",
    "    TL_lons = np.linspace(TL_lon, BR_lon - cell_size, lon_steps + 1)\n",
    "\n",
    "    disp_data = {}\n",
    "\n",
    "    for lat in TL_lats:\n",
    "        for lon in TL_lons:\n",
    "            data = get_disp_data_by_coords(lat, lon, cell_size)\n",
    "            disp_data.update(data)\n",
    "\n",
    "    print('Total dispensaries found: ', len(disp_data))\n",
    "    return disp_data\n",
    "\n",
    "\n",
    "def get_disp_dict(path):\n",
    "    \"\"\"\n",
    "    Performs a grid search across Washington for all dispensaries with an\n",
    "    account on Leafly and scrapes metadata for each\n",
    "    Input: relative path to raw data directory\n",
    "    Output: Index formatted JSON with one dictionary for each found dispensary\n",
    "    \"\"\"\n",
    "    filepath = path + 'dispensary_list.json'\n",
    "    \n",
    "    if os.path.isfile(filepath):\n",
    "        overwrite = input(\n",
    "            '''Initial dispensary list already exists. Scrape data again? y/n\\n \n",
    "            Note: this could take several minutes.''')\n",
    "        if overwrite.lower() != 'y':\n",
    "            with open(filepath) as json_file:\n",
    "                data = json.load(json_file)\n",
    "            return data\n",
    "    print(\"Beginning scrape...\")\n",
    "    \n",
    "    # WA State bounding coordinates\n",
    "    north = 49\n",
    "    west = -124.8\n",
    "    south = 45.4\n",
    "    east = -116.8\n",
    "    \n",
    "    data = get_rect_disp_data(north, west, south, east, cell_size=1.4)\n",
    "    \n",
    "    with open(filepath, 'w') as outfile:  \n",
    "        json.dump(data, outfile)\n",
    "        print('Scraped data written to {}'.format(filepath))\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_leafly_disp_data(path, disp_filename):\n",
    "    \"\"\"\n",
    "    Steps through all helper functions to scrape data from Leafly\n",
    "    Input: raw data path and desired filename for output\n",
    "    Output: JSON file containing scraped data\n",
    "    \"\"\"\n",
    "    disp_dict = get_disp_dict(path)\n",
    "    disp_data = scrape_leafly_disps(path, disp_filename, disp_dict)\n",
    "    return\n",
    "\n",
    "\n",
    "def get_demo_data(path, license_filename, demo_filename):\n",
    "    \"\"\"\n",
    "    Scrapes zip code based demographic data from washington.hometownlocator.com\n",
    "    for all zip codes containing a dispensary found in WSLCB license data\n",
    "    Input: relative path to raw data directory, license data filename, \n",
    "    demographics data filename\n",
    "    Output: saves demographic dataset to csv in raw data directory\n",
    "    \"\"\"\n",
    "    license_filepath = path + license_filename\n",
    "    demo_filepath = path + demo_filename\n",
    "\n",
    "    if os.path.isfile(demo_filepath):\n",
    "        overwrite = input(\n",
    "            '''Demographics file already exists. Scrape data again? y/n\\n \n",
    "            Note: this could take several minutes.''')\n",
    "        if overwrite.lower() != 'y':\n",
    "            return\n",
    "    \n",
    "    license_data = pd.read_excel(license_filepath, sheet_name=2, header=0)\n",
    "    zips = license_data['ZipCode'].astype(str).str[:5].unique()\n",
    "    demographics = pd.DataFrame()\n",
    "    \n",
    "    print(\"Beginning scrape...\")\n",
    "    for zip_code in sorted(zips):\n",
    "        url = f'https://washington.hometownlocator.com/zip-codes/data,zipcode,{zip_code}.cfm'\n",
    "        r = requests.get(url)\n",
    "        if 'table' in r.text:\n",
    "            df0, df1 = pd.read_html(url, index_col=0)[:2]\n",
    "            df0.columns = [str(zip_code)]\n",
    "            df1.columns = [str(zip_code)]\n",
    "            df = pd.concat([df0, df1], axis=0).T.dropna(axis=1)\n",
    "            df.drop(['INCOME', 'HOUSEHOLDS'], axis=1, inplace=True)\n",
    "            demographics = pd.concat([demographics, df])\n",
    "            print('Scraped {}/{} zips. Latest: {}'\n",
    "                  .format(len(demographics), len(zips), zip_code), end='\\r')\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            print(f'\\nNo data found for {zip_code}')\n",
    "        \n",
    "    demographics.to_csv(demo_filepath)\n",
    "    print('Scraped data written to {}'.format(demo_filepath))\n",
    "    return\n",
    "    \n",
    "    \n",
    "def download_dataset(url, path, filename):\n",
    "    \"\"\"\n",
    "    Downloads dataset from specified url and saves file to raw data directory\n",
    "    Input: url from which to retrieve data, filename to store data in\n",
    "    Output: dataset stored in raw data file directory\n",
    "    \"\"\"\n",
    "#     filepath = '../data/raw/{}'.format(filename)\n",
    "    filepath = path + filename\n",
    "    file_exists = os.path.isfile(filepath)\n",
    "    if file_exists:\n",
    "        overwrite = input('{} already exists. Update? y/n'.format(filename))\n",
    "        if overwrite.lower() != 'y':\n",
    "            return\n",
    "    print(\"Beginning file download...\")\n",
    "    r = requests.get(url)\n",
    "    if not r.ok:\n",
    "        print('Download failed')\n",
    "        return\n",
    "    with open(filepath, 'wb') as f:  \n",
    "        f.write(r.content)\n",
    "    print('File written to {}\\n'.format(filepath))\n",
    "    return\n",
    "    \n",
    "    \n",
    "def get_sales_data(path, sales_filename, license_filename):\n",
    "    \"\"\"\n",
    "    Gets links for most up-to-date dispensary sales and license information\n",
    "    from WSLCB and downloads datasets\n",
    "    Input:\n",
    "    Output: downloaded files to raw data directory\n",
    "    \"\"\"\n",
    "    # Get urls for most up-to-date sales and license data\n",
    "    url = 'https://lcb.wa.gov/records/frequently-requested-lists'\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = soup.find_all('a')\n",
    "        for link in links:\n",
    "            if 'Traceability' in link.text:\n",
    "                sales_url = link['href']\n",
    "                print(f'\\nLatest sales data found:\\n{sales_url}')\n",
    "                #filename = 'sales_data.xlsx'\n",
    "                download_dataset(sales_url, path, sales_filename)\n",
    "            elif 'Applicants' in link.text:\n",
    "                licenses_url = link['href']\n",
    "                print(f'\\nLatest license data found:\\n{licenses_url}')\n",
    "                #filename = 'license_data.xls'\n",
    "                download_dataset(licenses_url, path, license_filename)\n",
    "    else:\n",
    "        print('Failed to download sales data')\n",
    "\n",
    "    return\n",
    "\n",
    "    \n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that download data several\n",
    "    sources and saves those datasets to the data/raw directory.\n",
    "    \"\"\"\n",
    "    path = '../data/raw/'\n",
    "    \n",
    "    sales_filename = 'sales_data.xlsx'\n",
    "    license_filename = 'license_data.xls'\n",
    "    demo_filename = 'demographics.csv'\n",
    "    disp_filename = 'dispensary_data.json'\n",
    "    \n",
    "    get_sales_data(path, sales_filename, license_filename)\n",
    "    get_demo_data(path, license_filename, demo_filename)\n",
    "    get_leafly_disp_data(path, disp_filename)\n",
    "    \n",
    "    print('\\nData acquisition complete.\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latest license data found:\n",
      "https://lcb.wa.gov/sites/default/files/publications/Public_Records/2019/MarijuanaApplicants.xls\n",
      "license_data.xls already exists. Update? y/ny\n",
      "Beginning file download...\n",
      "File written to ../data/raw/license_data.xls\n",
      "\n",
      "Latest sales data found:\n",
      "https://lcb.wa.gov/sites/default/files/publications/Marijuana/sales_activity/2019-04-10-MJ-Sales-Activity-by-License-Number-Traceability-Contingency-Reporting.xlsx\n",
      "sales_data.xlsx already exists. Update? y/ny\n",
      "Beginning file download...\n",
      "File written to ../data/raw/sales_data.xlsx\n",
      "Demographics file already exists. Scrape data again? y/nn\n",
      "Initial dispensary list already exists. Scrape data again? y/nn\n",
      "Dispensaries data dict already exists. Scrape data again? y/nn\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub the Data\n",
    "\n",
    "*Look through the raw data files and see what you will need to do to them in order to have a workable data set. If your source data is already well-formatted, you may want to ask yourself why it hasn't already been analyzed and what other people may have overlooked when they were working on it. Are there other data sources that might give you more insights on some of the data you have here?*\n",
    "\n",
    "*The end goal of this step is to produce a [design matrix](https://en.wikipedia.org/wiki/Design_matrix), containing one column for every variable that you are modeling, including a column for the outputs, and one row for every observation in your data set. It needs to be in a format that won't cause any problems as you visualize and model your data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/features/build_features.py\n",
    "# !pip install fuzzywuzzy\n",
    "# !pip install python-Levenshtein\n",
    "\n",
    "# Imports\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "def build_features(path, filename):\n",
    "    pass\n",
    "    \n",
    "def join_cleaned_data(path, filename):\n",
    "    pass\n",
    "    \n",
    "def join_sales_data(path, filename):\n",
    "    \"\"\"\n",
    "    Loads sales and license data files and joins them into one table\n",
    "    Also creates a column with zip code as a 5 digit string for later use\n",
    "    Input:\n",
    "    Output: returns merged dataframe\n",
    "    \"\"\"\n",
    "    filepath = path['raw'] + filename['raw_sales']\n",
    "    disp_sales_data = pd.read_excel(filepath, sheet_name=0, header=3)\n",
    "    disp_sales_data.rename(columns={'License Number':'License #'}, inplace=True)\n",
    "    disp_sales_data.set_index(keys='License #', inplace=True)\n",
    "\n",
    "    filepath = path['raw'] + filename['raw_license']\n",
    "    license_data = pd.read_excel(filepath, sheet_name=2, header=0, index_col=1)\n",
    "\n",
    "    sales_data = pd.merge(disp_sales_data, license_data, how='left', on='License #')\n",
    "    \n",
    "    return sales_data  \n",
    "    \n",
    "def clean_wslcb_data(path, filename):\n",
    "    \"\"\"\n",
    "    Loads, cleans, and joins raw license and sales data from WSLCB.\n",
    "    Input: dictionaries containing paths and filenames for input/output files\n",
    "    Output: a cleaned and pickled dataframe of data from WSLCB\n",
    "    \"\"\"\n",
    "    raw_filename = path['raw'] + filename['raw_']\n",
    "    int_filename = path['interim'] + filename['int_leafly']\n",
    "    \n",
    "    \n",
    "    \n",
    "    sales = join_sales_data(path, filename)\n",
    "    sales['zip_code'] = sales['ZipCode'].astype(str).str[:5]\n",
    "\n",
    "        \n",
    "    sales.to_pickle(int_filename)\n",
    "    return\n",
    "    \n",
    "def clean_demographic_data(path, filename):\n",
    "    \"\"\"\n",
    "    Loads and cleans raw demographic data.\n",
    "    Input: dictionaries containing paths and filenames for input/output files\n",
    "    Output: a cleaned and pickled dataframe of demographic\n",
    "    \"\"\"\n",
    "    raw_filename = path['raw'] + filename['raw_demo']\n",
    "    int_filename = path['interim'] + filename['int_demo']\n",
    "    \n",
    "    demo = pd.read_csv(raw_filename)\n",
    "        \n",
    "    demo.to_pickle(int_filename)\n",
    "    return\n",
    "\n",
    "\n",
    "def make_key(row, fields):\n",
    "    \"\"\"\n",
    "    Constructs a key from a simplified composite of a dispensary's name, city,\n",
    "    and address\n",
    "    Input: dataframe row (from apply function) along with a list of names of \n",
    "    fields to get key information from\n",
    "    Output: lowercase string containing unique tokens from each of the fields \n",
    "    with punctuation removed and commonly shortened terms abbreviated\n",
    "    \"\"\"\n",
    "    joined = ' '.join([row[str(f)] for f in fields if isinstance(f, str)]).lower()\n",
    "    remove = list('!@#$%^&*()-_+,./<>?[]{}')\n",
    "    for char in remove:\n",
    "        joined = joined.replace(char, '')\n",
    "    abbr = {'street': 'st',\n",
    "            'road': 'rd',\n",
    "            'way': 'wy',\n",
    "            'drive': 'dr',\n",
    "            'highway': 'hwy',\n",
    "            'avenue': 'ave',\n",
    "            'boulevard': 'blvd',\n",
    "            'suite': 'ste',\n",
    "            'lane': 'ln',\n",
    "            'north': 'n',\n",
    "            'east': 'e',\n",
    "            'south': 's',\n",
    "            'west': 'w',\n",
    "            'northeast': 'ne',\n",
    "            'southeast': 'se',\n",
    "            'southwest': 'sw',\n",
    "            'northwest': 'nw',\n",
    "           }\n",
    "    for token in abbr:\n",
    "        joined = joined.replace(token, abbr[token])\n",
    "    key = ' '.join(sorted(list(set(joined.split()))))\n",
    "    return key\n",
    "\n",
    "\n",
    "def clean_leafly_data(path, filename):\n",
    "    \"\"\"\n",
    "    Loads and cleans raw data scraped from Leafly.\n",
    "    Input: dictionaries containing paths and filenames for input/output files\n",
    "    Output: a cleaned and pickled dataframe of data scraped from Leafly\n",
    "    \"\"\"\n",
    "    raw_filename = path['raw'] + filename['raw_leafly']\n",
    "    int_filename = path['interim'] + filename['int_leafly']\n",
    "\n",
    "    # Read data from file to dataframe\n",
    "    data = pd.read_json(raw_filename, orient='index')\n",
    "\n",
    "    # Filter to only Washington entries\n",
    "    f = data['formattedShortLocation'].str.contains('WA')\n",
    "    data = data[f]\n",
    "\n",
    "    # Cast menu update as datetime\n",
    "    data['lastMenuUpdate'] = pd.to_datetime(\n",
    "        data['lastMenuUpdate'], errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "    # Extact lat/lon coordinates into their own feature\n",
    "    data[['latitude', 'longitude']] = data['location'].apply(pd.Series)\n",
    "\n",
    "    # Cast categorical features as such\n",
    "    cat_cols = ['ada accessible', 'atm', 'debit cards accepted', 'medical', \n",
    "                'recreational', 'storefront', 'ufcw discount', \n",
    "                'veteran discount']\n",
    "    data[cat_cols] = data[cat_cols].astype('category')\n",
    "\n",
    "    # Construct composite key for joining with sales data\n",
    "    key_fields = ['name', 'city', 'address1']\n",
    "    data['key'] = data.apply(make_key, axis=1, fields=key_fields)\n",
    "    \n",
    "    # Pickle and return dataframe\n",
    "    data.to_pickle(int_filename)\n",
    "    return data\n",
    "\n",
    "    \n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/raw, \n",
    "    cleans them, and converts the data into a design matrix that is ready\n",
    "    for modeling.\n",
    "    \"\"\"\n",
    "    path = {\n",
    "        'raw': '../data/raw/',\n",
    "        'interim': '../data/interim/',\n",
    "        'processed': '../data/processed/'\n",
    "    }\n",
    "    \n",
    "    filename = {\n",
    "        'raw_leafly': 'dispensary_data.json',\n",
    "        'raw_demo': 'demographics.csv',\n",
    "        'raw_license': 'license_data.xls',\n",
    "        'raw_sales': 'sales_data.xlsx',\n",
    "        'int_leafly': 'leafly.pkl',\n",
    "        'int_demo': 'demographics.pkl',\n",
    "        'int_sales': 'sales.pkl',\n",
    "        'processed': 'data.pkl'\n",
    "    }\n",
    "    \n",
    "    clean_leafly_data(path, filename)\n",
    "    clean_demographic_data(path, filename)\n",
    "    clean_wslcb_data(path, filename)\n",
    "    join_cleaned_data(path, filename)\n",
    "    build_features(path, filename)\n",
    "    \n",
    "    print('\\nData acquisition complete.\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Setup (delete from here down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 50)\n",
    "path = {\n",
    "    'raw': '../data/raw/',\n",
    "    'interim': '../data/interim/',\n",
    "    'processed': '../data/processed/'\n",
    "}\n",
    "    \n",
    "filename = {\n",
    "    'raw_leafly': 'dispensary_data.json',\n",
    "    'raw_demo': 'demographics.csv',\n",
    "    'raw_license': 'license_data.xls',\n",
    "    'raw_sales': 'sales_data.xlsx',\n",
    "    'int_leafly': 'leafly.pkl',\n",
    "    'int_demo': 'demographics.pkl',\n",
    "    'int_sales': 'sales.pkl',\n",
    "    'processed': 'data.pkl'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Leafly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_filename = path['raw'] + filename['raw_leafly']\n",
    "int_filename = path['interim'] + filename['int_leafly']\n",
    "\n",
    "data = pd.read_json(raw_filename, orient='index')\n",
    "\n",
    "# Filter to only Washington entries\n",
    "f = data['formattedShortLocation'].str.contains('WA')\n",
    "data = data[f]\n",
    "\n",
    "# Cast menu update as datetime\n",
    "data['lastMenuUpdate'] = pd.to_datetime(\n",
    "    data['lastMenuUpdate'], errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "# Extact lat/lon coordinates into their own feature\n",
    "data[['latitude', 'longitude']] = data['location'].apply(pd.Series)\n",
    "\n",
    "# Cast categorical features as such\n",
    "cat_cols = ['ada accessible', 'atm', 'debit cards accepted', 'medical', \n",
    "            'recreational', 'storefront', 'ufcw discount', 'veteran discount']\n",
    "data[cat_cols] = data[cat_cols].astype('category')\n",
    "\n",
    "# Construct composite key for joining with sales data\n",
    "key_fields = ['name', 'city', 'address1']\n",
    "data['key'] = data.apply(make_key, axis=1, fields=key_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Sales and License data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reporting Period</th>\n",
       "      <th>Total Sales</th>\n",
       "      <th>Excise Tax Due</th>\n",
       "      <th>Tradename</th>\n",
       "      <th>UBI</th>\n",
       "      <th>Street Address</th>\n",
       "      <th>Suite/Rm</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>PrivDesc</th>\n",
       "      <th>PrivilegeStatus</th>\n",
       "      <th>DateIssued</th>\n",
       "      <th>DayPhone</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>License #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414884</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>237642.93</td>\n",
       "      <td>87927.88</td>\n",
       "      <td>#hashtag</td>\n",
       "      <td>6.033581e+15</td>\n",
       "      <td>3540 STONE WAY N</td>\n",
       "      <td></td>\n",
       "      <td>SEATTLE</td>\n",
       "      <td>WA</td>\n",
       "      <td>KING</td>\n",
       "      <td>981038924.0</td>\n",
       "      <td>MARIJUANA RETAILER</td>\n",
       "      <td>ACTIVE (ISSUED)</td>\n",
       "      <td>2018-11-29</td>\n",
       "      <td>3.608184e+09</td>\n",
       "      <td>98103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423413</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>463721.35</td>\n",
       "      <td>171576.90</td>\n",
       "      <td>112th street cannabis</td>\n",
       "      <td>6.033492e+15</td>\n",
       "      <td>5809 112TH ST E BLDG B</td>\n",
       "      <td></td>\n",
       "      <td>PUYALLUP</td>\n",
       "      <td>WA</td>\n",
       "      <td>PIERCE</td>\n",
       "      <td>983734323.0</td>\n",
       "      <td>MARIJUANA RETAILER</td>\n",
       "      <td>ACTIVE (ISSUED)</td>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>2.069924e+09</td>\n",
       "      <td>98373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364799</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>217209.85</td>\n",
       "      <td>80367.64</td>\n",
       "      <td>2020 solutions edmonds</td>\n",
       "      <td>6.035723e+15</td>\n",
       "      <td>7207 212TH ST SW</td>\n",
       "      <td></td>\n",
       "      <td>EDMONDS</td>\n",
       "      <td>WA</td>\n",
       "      <td>SNOHOMISH</td>\n",
       "      <td>980207735.0</td>\n",
       "      <td>MARIJUANA RETAILER</td>\n",
       "      <td>ACTIVE (ISSUED)</td>\n",
       "      <td>2019-03-26</td>\n",
       "      <td>3.609155e+09</td>\n",
       "      <td>98020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422239</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>60398.07</td>\n",
       "      <td>22347.29</td>\n",
       "      <td>2020 solutions ephrata</td>\n",
       "      <td>6.035723e+15</td>\n",
       "      <td>1615 BASIN ST SW</td>\n",
       "      <td></td>\n",
       "      <td>EPHRATA</td>\n",
       "      <td>WA</td>\n",
       "      <td>GRANT</td>\n",
       "      <td>988232134.0</td>\n",
       "      <td>MARIJUANA RETAILER</td>\n",
       "      <td>ACTIVE (ISSUED)</td>\n",
       "      <td>2018-12-08</td>\n",
       "      <td>3.609155e+09</td>\n",
       "      <td>98823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422363</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>27246.27</td>\n",
       "      <td>10081.12</td>\n",
       "      <td>2020 solutions soap lake</td>\n",
       "      <td>6.035723e+15</td>\n",
       "      <td>261 STATE HWY 28 WEST</td>\n",
       "      <td></td>\n",
       "      <td>SOAP LAKE</td>\n",
       "      <td>WA</td>\n",
       "      <td>GRANT</td>\n",
       "      <td>988510000.0</td>\n",
       "      <td>MARIJUANA RETAILER</td>\n",
       "      <td>ACTIVE (ISSUED)</td>\n",
       "      <td>2018-12-08</td>\n",
       "      <td>3.609155e+09</td>\n",
       "      <td>98851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Reporting Period  Total Sales  Excise Tax Due  \\\n",
       "License #                                                 \n",
       "414884          2019-02-01    237642.93        87927.88   \n",
       "423413          2019-02-01    463721.35       171576.90   \n",
       "364799          2019-02-01    217209.85        80367.64   \n",
       "422239          2019-02-01     60398.07        22347.29   \n",
       "422363          2019-02-01     27246.27        10081.12   \n",
       "\n",
       "                          Tradename           UBI  \\\n",
       "License #                                           \n",
       "414884                     #hashtag  6.033581e+15   \n",
       "423413        112th street cannabis  6.033492e+15   \n",
       "364799       2020 solutions edmonds  6.035723e+15   \n",
       "422239       2020 solutions ephrata  6.035723e+15   \n",
       "422363     2020 solutions soap lake  6.035723e+15   \n",
       "\n",
       "                           Street Address                   Suite/Rm  \\\n",
       "License #                                                              \n",
       "414884     3540 STONE WAY N                                            \n",
       "423413     5809 112TH ST E BLDG B                                      \n",
       "364799     7207 212TH ST SW                                            \n",
       "422239     1615 BASIN ST SW                                            \n",
       "422363     261 STATE HWY 28 WEST                                       \n",
       "\n",
       "                               City State     County      ZipCode  \\\n",
       "License #                                                           \n",
       "414884     SEATTLE                     WA       KING  981038924.0   \n",
       "423413     PUYALLUP                    WA     PIERCE  983734323.0   \n",
       "364799     EDMONDS                     WA  SNOHOMISH  980207735.0   \n",
       "422239     EPHRATA                     WA      GRANT  988232134.0   \n",
       "422363     SOAP LAKE                   WA      GRANT  988510000.0   \n",
       "\n",
       "                                      PrivDesc  PrivilegeStatus DateIssued  \\\n",
       "License #                                                                    \n",
       "414884     MARIJUANA RETAILER                   ACTIVE (ISSUED) 2018-11-29   \n",
       "423413     MARIJUANA RETAILER                   ACTIVE (ISSUED) 2018-12-22   \n",
       "364799     MARIJUANA RETAILER                   ACTIVE (ISSUED) 2019-03-26   \n",
       "422239     MARIJUANA RETAILER                   ACTIVE (ISSUED) 2018-12-08   \n",
       "422363     MARIJUANA RETAILER                   ACTIVE (ISSUED) 2018-12-08   \n",
       "\n",
       "               DayPhone zip_code  \n",
       "License #                         \n",
       "414884     3.608184e+09    98103  \n",
       "423413     2.069924e+09    98373  \n",
       "364799     3.609155e+09    98020  \n",
       "422239     3.609155e+09    98823  \n",
       "422363     3.609155e+09    98851  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 417 entries, 71368 to 428005\n",
      "Data columns (total 16 columns):\n",
      "Reporting Period    417 non-null datetime64[ns]\n",
      "Total Sales         417 non-null float64\n",
      "Excise Tax Due      417 non-null float64\n",
      "Tradename           415 non-null object\n",
      "UBI                 415 non-null float64\n",
      "Street Address      415 non-null object\n",
      "Suite/Rm            415 non-null object\n",
      "City                415 non-null object\n",
      "State               415 non-null object\n",
      "County              415 non-null object\n",
      "ZipCode             415 non-null float64\n",
      "PrivDesc            415 non-null object\n",
      "PrivilegeStatus     415 non-null object\n",
      "DateIssued          410 non-null datetime64[ns]\n",
      "DayPhone            415 non-null float64\n",
      "zip_code            417 non-null object\n",
      "dtypes: datetime64[ns](2), float64(5), object(9)\n",
      "memory usage: 55.4+ KB\n"
     ]
    }
   ],
   "source": [
    "sales_data = join_sales_data(path, filename)\n",
    "\n",
    "sales_data['zip_code'] = sales_data['ZipCode'].astype(str).str[:5]\n",
    "sales_data['Tradename'] = sales_data['Tradename'].str.strip().str.lower()\n",
    "\n",
    "sales_data['Reporting Period'] += '-01'\n",
    "date_cols = ['Reporting Period', 'DateIssued']\n",
    "for col in date_cols:\n",
    "    sales_data[col] = pd.to_datetime(sales_data[col], errors='coerce', \n",
    "                                     yearfirst=True, infer_datetime_format=True)\n",
    "\n",
    "recent = sales_data['Reporting Period'] == sales_data['Reporting Period'].max()\n",
    "recent_sales = sales_data[recent]\n",
    "display(recent_sales.sort_values(by='Tradename').head())\n",
    "recent_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before moving on to exploratory analysis, write down some notes about challenges encountered while working with this data that might be helpful for anyone else (including yourself) who may work through this later on.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data\n",
    "\n",
    "*Before you start exploring the data, write out your thought process about what you're looking for and what you expect to find. Take a minute to confirm that your plan actually makes sense.*\n",
    "\n",
    "*Calculate summary statistics and plot some charts to give you an idea what types of useful relationships might be in your dataset. Use these insights to go back and download additional data or engineer new features if necessary. Not now though... remember we're still just trying to finish the MVP!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/visualization/visualize.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from \n",
    "    data/processed, calculates descriptive statistics for the population,\n",
    "    and plots charts that visualize interesting relationships between \n",
    "    features.\n",
    "    \"\"\"\n",
    "    # data = load_features('data/processed')\n",
    "    # describe_features(data, 'reports/')\n",
    "    # generate_charts(data, 'reports/figures/')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What did you learn? What relationships do you think will be most helpful as you build your model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the Data\n",
    "\n",
    "*Describe the algorithm or algorithms that you plan to use to train with your data. How do these algorithms work? Why are they good choices for this data and problem space?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/models/train_model.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from \n",
    "    data/processed, calculates descriptive statistics for the population,\n",
    "    and plots charts that visualize interesting relationships between \n",
    "    features.\n",
    "    \"\"\"\n",
    "    # data = load_features('data/processed/')\n",
    "    # train, test = train_test_split(data)\n",
    "    # save_train_test(train, test, 'data/processed/')\n",
    "    # model = build_model()\n",
    "    # model.fit(train)\n",
    "    # save_model(model, 'models/')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/models/predict_model.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from \n",
    "    data/processed, calculates descriptive statistics for the population,\n",
    "    and plots charts that visualize interesting relationships between\n",
    "    features.\n",
    "    \"\"\"\n",
    "    # test_X, test_y = load_test_data('data/processed')\n",
    "    # trained_model = load_model('models/')\n",
    "    # predictions = trained_model.predict(test_X)\n",
    "    # metrics = evaluate(test_y, predictions)\n",
    "    # save_metrics('reports/')\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write down any thoughts you may have about working with these algorithms on this data. What other ideas do you want to try out as you iterate on this pipeline?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret the Model\n",
    "\n",
    "_Write up the things you learned, and how well your model performed. Be sure address the model's strengths and weaknesses. What types of data does it handle well? What types of observations tend to give it a hard time? What future work would you or someone reading this might want to do, building on the lessons learned and tools developed in this project?_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project2)",
   "language": "python",
   "name": "project2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

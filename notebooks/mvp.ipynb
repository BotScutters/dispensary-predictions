{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "*A problem well stated is a problem half-solved.*\n",
    "\n",
    "*This is your space to describe your intentions for the project, before writing a single line of code. What are you studying? What are you hoping to build? If you can't explain that clearly before you start digging into the data, you're going to have a hard time planning where to go with this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain the Data\n",
    "\n",
    "* Describe your data sources here and explain why they are relevant to the problem you are trying to solve.*\n",
    "\n",
    "* Your code should download the data and save it in data/raw. If you've got the data from an offline source, describe where it came from and what the files look like. Don't do anything to the raw data files just yet; that comes in the next step.*\n",
    "\n",
    "* After completing this step, be sure to edit `references/data_dictionary` to include descriptions of where you obtained your data and what information it contains.*\n",
    "\n",
    "Sales dataset is downloaded from the Washington State Liquor and Cannabis Board (WCLSB) website as an Excel spreadsheet and contains monthly revenue reportings for every cannabis dispensary in the state dating back to November 2017. The only identifying feature for each dispensary in this spreadsheet is the license number, so it will have to be joined with other data using this as a key to be useful.\n",
    "\n",
    "The licensed businesses dataset is also downloaded from the WCLSB website, and contains metadata about each dispensary, including the license numbers of each dispensary along with other information such as address and license type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/data/make_dataset.py\n",
    "\n",
    "# Imports\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Helper functions\n",
    "def parse_products(text):\n",
    "    '''\n",
    "    Parses string of products into dictionary of products with counts\n",
    "    Input: string of products as scraped from Leafly dispensary page\n",
    "    Output: dictionary of {product: count} relationships\n",
    "    '''\n",
    "    repl = ['(', ')']\n",
    "    for char in repl:\n",
    "        text = text.replace(char, '')\n",
    "    prod_list = text.split('\\n')\n",
    "    prod_list = [prod.strip().lower() for prod in prod_list]\n",
    "    prod_dict = {}\n",
    "    for i, element in enumerate(prod_list):\n",
    "        if element.isnumeric():\n",
    "            prod_dict[prod_list[i - 1]] = int(element)\n",
    "        elif 'difference' in element:\n",
    "            pass\n",
    "        else:\n",
    "            prod_dict[element.strip()] = 0\n",
    "    return prod_dict\n",
    "\n",
    "\n",
    "def scrape_disp(disp, driver, user_agent):\n",
    "    \"\"\"\n",
    "    Scrapes dispensary-specific page on leafly for additional data and adds it\n",
    "    to existing dictionary dataset\n",
    "    Input: dictionary containing metadata for a single dispensary\n",
    "    Output: dictionary with additional metadata for given dispensary\n",
    "    \"\"\"\n",
    "    url = 'https://www.leafly.com/dispensary-info/'\n",
    "    slug = disp['slug']\n",
    "    url += slug\n",
    "    \n",
    "    if 'OR' in disp['formattedShortLocation']:\n",
    "        return {}\n",
    "    \n",
    "    response  = requests.get(url, headers=user_agent)\n",
    "    if not response.ok:\n",
    "        print('Connection to {} failed'.format(disp['name']))\n",
    "        return {}\n",
    "    \n",
    "    # Open page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Confirm over 21\n",
    "    try:\n",
    "        yes_button = driver.find_element_by_xpath('//button[text()=\"Yes\"]')\n",
    "        yes_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Scrape categoricals\n",
    "    try:\n",
    "        cat_selector = driver.find_element_by_class_name('jsx-4153842418')\n",
    "        # cat_selector = driver.find_element_by_tag_name('ul')\n",
    "        items = cat_selector.find_elements_by_tag_name(\"li\")\n",
    "        categories = {item.text.lower(): True for item in items}\n",
    "        disp.update(categories)\n",
    "    except:\n",
    "        print('Failed to scrape categories for {}'.format(disp['name']))\n",
    "        pass\n",
    "\n",
    "    # Scrape products\n",
    "    try:\n",
    "        products = driver.find_elements_by_class_name('jsx-1433915045')\n",
    "        products_text = products[0].text\n",
    "        product_dict = parse_products(products_text)\n",
    "        disp.update(product_dict)\n",
    "    except:\n",
    "        print('Failed to scrape products for {}'.format(disp['name']))\n",
    "        pass\n",
    "    \n",
    "    print('Successfully scraped {}'.format(disp['name']))\n",
    "    return disp\n",
    "\n",
    "\n",
    "def scrape_leafly_disps(path, disp_data_filename, data):\n",
    "    \"\"\"\n",
    "    Gets JSON file of data on dispensaries from Leafly, either by loading\n",
    "    pre-existing file or by re-scraping Leafly\n",
    "    Input: path and filename for output file, index  of basic dispensary metadata\n",
    "    Output: Index formatted JSON with one dictionary for each found dispensary\n",
    "    \"\"\"\n",
    "#     filepath = '../data/raw/dispensary_data.json'\n",
    "    filepath = path + disp_data_filename\n",
    "    if os.path.isfile(filepath):\n",
    "        overwrite = input(\n",
    "            '''Dispensaries data dict already exists. Scrape data again? y/n\\n\n",
    "            Note: this could take several minutes.''')\n",
    "        if overwrite.lower() != 'y':\n",
    "            with open(filepath) as json_file:\n",
    "                data = json.load(json_file)\n",
    "            return data\n",
    "\n",
    "    print(\"Beginning scrape...\")\n",
    "    ua = UserAgent()\n",
    "    user_agent = {'User-agent': ua.random}\n",
    "    chromedriver = \"/Applications/chromedriver\"\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "    for disp in data:\n",
    "        new_data = scrape_disp(data[disp], driver, user_agent)\n",
    "        data[disp].update(new_data)\n",
    "    \n",
    "    with open(filepath, 'w') as outfile:  \n",
    "        json.dump(data, outfile)\n",
    "        print('Scraped data written to {}'.format(filepath))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def retry(TL_lat, TL_lon, cell_size):\n",
    "    '''\n",
    "    If request hits Leafly API limit, split cell into 4 subcells and retry\n",
    "    Input: Lat/lon coordinates for top left of map and optionally a size for\n",
    "    the map area (defaults to 0.5)\n",
    "    Output: dictionary of dictionaries containing metadata for each dispensary \n",
    "    found in map area\n",
    "    '''\n",
    "    TL_lats = [TL_lat, TL_lat - 0.4 * cell_size]\n",
    "    TL_lons = [TL_lat, TL_lat + 0.4 * cell_size]\n",
    "    disp_data = {}\n",
    "    for lat, lon in zip(TR_lats, TR_lons):\n",
    "        data = get_disp_data_by_coords(lat, lon, cell_size=0.6 * cell_size)\n",
    "        disp_data.update(data)\n",
    "    return disp_data\n",
    "\n",
    "\n",
    "def get_disp_data_by_coords(TL_lat, TL_lon, cell_size=0.5):\n",
    "    \"\"\"\n",
    "    Performs search for all dispensaries within a map region on Leafly\n",
    "    Input: Lat/lon coordinates for top left of map and optionally a size for\n",
    "    the map area (defaults to 0.5)\n",
    "    Output: dictionary of dictionaries containing metadata for each dispensary \n",
    "    found in map area\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    BR_lat = TL_lat - cell_size\n",
    "    BR_lon = TL_lon + cell_size\n",
    "    coords = TL_lat, TL_lon, BR_lat, BR_lon\n",
    "    \n",
    "    url = (\n",
    "        'https://web-finder.leafly.com/api/searchThisArea?topLeftLat={}&topLeftLon={}&bottomRightLat={}&bottomRightLon={}&userLat=47.6&userLon=-122.3'\n",
    "        ).format(TL_lat, TL_lon, BR_lat, BR_lon)\n",
    "    \n",
    "    # Scrape\n",
    "    time.sleep(.5+2*random.random())\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print('Leafly search failed at {}'.format(coords))\n",
    "        return {}\n",
    "    disps = r.json()\n",
    "    \n",
    "    # Parse\n",
    "    fields = ['name', 'address1', 'address2', 'city', 'location', 'phone',\n",
    "              'formattedShortLocation', 'medical', 'recreational', 'tier', \n",
    "              'lastMenuUpdate', 'starRating', 'numberOfReviews', 'slug']\n",
    "\n",
    "    disp_data = {\n",
    "        d['name']: {k: d[k] for k in fields} for d in disps['dispensaries']}\n",
    "    entries = len(disp_data)\n",
    "    \n",
    "    # Check results; retry if necessary and return data\n",
    "    if entries > 200:\n",
    "        return retry(TR_lat, TR_lon, cell_size)\n",
    "    elif entries < 1:\n",
    "#         print('no results at {}'.format(coords))\n",
    "        return {}\n",
    "    else:\n",
    "#         print('{} results found at {}'.format(len(disp_data), coords))\n",
    "        return disp_data\n",
    "    \n",
    "    \n",
    "def get_rect_disp_data(TL_lat, TL_lon, BR_lat, BR_lon, cell_size=0.5):\n",
    "    \"\"\"\n",
    "    Performs grid search on sub-rectangles with slight overlap, gathering data \n",
    "    on each cell\n",
    "    Input: lat/lon coords of top left and bottom right corners, as well as \n",
    "    optional cell size parameter (defaults to 0.5)\n",
    "    Output: dictionary of dictionaries representing all dispensaries in\n",
    "    rectangle\n",
    "    \"\"\"\n",
    "    coords = TL_lat, TL_lon, BR_lat, BR_lon\n",
    "    max_step = 0.8 * cell_size\n",
    "    lat_steps = np.ceil((TL_lat - BR_lat - cell_size) / max_step)\n",
    "    lon_steps = np.ceil((BR_lon - TL_lon - cell_size) / max_step)\n",
    "\n",
    "    TL_lats = np.linspace(TL_lat, BR_lat + cell_size, lat_steps + 1)\n",
    "    TL_lons = np.linspace(TL_lon, BR_lon - cell_size, lon_steps + 1)\n",
    "\n",
    "    disp_data = {}\n",
    "\n",
    "    for lat in TL_lats:\n",
    "        for lon in TL_lons:\n",
    "            data = get_disp_data_by_coords(lat, lon, cell_size)\n",
    "            disp_data.update(data)\n",
    "\n",
    "    print('Total dispensaries found: ', len(disp_data))\n",
    "    return disp_data\n",
    "\n",
    "\n",
    "def get_disp_dict(path):\n",
    "    \"\"\"\n",
    "    Performs a grid search across Washington for all dispensaries with an\n",
    "    account on Leafly and scrapes metadata for each\n",
    "    Input: relative path to raw data directory\n",
    "    Output: Index formatted JSON with one dictionary for each found dispensary\n",
    "    \"\"\"\n",
    "    filepath = path + 'dispensary_list.json'\n",
    "    \n",
    "    if os.path.isfile(filepath):\n",
    "        overwrite = input(\n",
    "            '''Initial dispensary list already exists. Scrape data again? y/n\\n \n",
    "            Note: this could take several minutes.''')\n",
    "        if overwrite.lower() != 'y':\n",
    "            with open(filepath) as json_file:\n",
    "                data = json.load(json_file)\n",
    "            return data\n",
    "    print(\"Beginning scrape...\")\n",
    "    \n",
    "    # WA State bounding coordinates\n",
    "    north = 49\n",
    "    west = -124.8\n",
    "    south = 45.4\n",
    "    east = -116.8\n",
    "    \n",
    "    data = get_rect_disp_data(north, west, south, east, cell_size=1.4)\n",
    "    \n",
    "    with open(filepath, 'w') as outfile:  \n",
    "        json.dump(data, outfile)\n",
    "        print('Scraped data written to {}'.format(filepath))\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_leafly_disp_data(path, disp_filename):\n",
    "    \"\"\"\n",
    "    Steps through all helper functions to scrape data from Leafly\n",
    "    Input: raw data path and desired filename for output\n",
    "    Output: JSON file containing scraped data\n",
    "    \"\"\"\n",
    "    disp_dict = get_disp_dict(path)\n",
    "    disp_data = scrape_leafly_disps(path, disp_filename, disp_dict)\n",
    "    return\n",
    "\n",
    "\n",
    "def get_demo_data(path, license_filename, demo_filename):\n",
    "    \"\"\"\n",
    "    Scrapes zip code based demographic data from washington.hometownlocator.com\n",
    "    for all zip codes containing a dispensary found in WSLCB license data\n",
    "    Input: relative path to raw data directory, license data filename, \n",
    "    demographics data filename\n",
    "    Output: saves demographic dataset to csv in raw data directory\n",
    "    \"\"\"\n",
    "    license_filepath = path + license_filename\n",
    "    demo_filepath = path + demo_filename\n",
    "\n",
    "    if os.path.isfile(demo_filepath):\n",
    "        overwrite = input(\n",
    "            '''Demographics file already exists. Scrape data again? y/n\\n \n",
    "            Note: this could take several minutes.''')\n",
    "        if overwrite.lower() != 'y':\n",
    "            return\n",
    "    \n",
    "    license_data = pd.read_excel(license_filepath, sheet_name=2, header=0)\n",
    "    zips = license_data['ZipCode'].astype(str).str[:5].unique()\n",
    "    demographics = pd.DataFrame()\n",
    "    \n",
    "    print(\"Beginning scrape...\")\n",
    "    for zip_code in sorted(zips):\n",
    "        url = f'https://washington.hometownlocator.com/zip-codes/data,zipcode,{zip_code}.cfm'\n",
    "        r = requests.get(url)\n",
    "        if 'table' in r.text:\n",
    "            df0, df1 = pd.read_html(url, index_col=0)[:2]\n",
    "            df0.columns = [str(zip_code)]\n",
    "            df1.columns = [str(zip_code)]\n",
    "            df = pd.concat([df0, df1], axis=0).T.dropna(axis=1)\n",
    "            df.drop(['INCOME', 'HOUSEHOLDS'], axis=1, inplace=True)\n",
    "            demographics = pd.concat([demographics, df])\n",
    "            print('Scraped {}/{} zips. Latest: {}'\n",
    "                  .format(len(demographics), len(zips), zip_code), end='\\r')\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            print(f'\\nNo data found for {zip_code}')\n",
    "        \n",
    "    demographics.to_csv(demo_filepath)\n",
    "    print('Scraped data written to {}'.format(demo_filepath))\n",
    "    return\n",
    "    \n",
    "    \n",
    "def download_dataset(url, path, filename):\n",
    "    \"\"\"\n",
    "    Downloads dataset from specified url and saves file to raw data directory\n",
    "    Input: url from which to retrieve data, filename to store data in\n",
    "    Output: dataset stored in raw data file directory\n",
    "    \"\"\"\n",
    "#     filepath = '../data/raw/{}'.format(filename)\n",
    "    filepath = path + filename\n",
    "    file_exists = os.path.isfile(filepath)\n",
    "    if file_exists:\n",
    "        overwrite = input('{} already exists. Update? y/n'.format(filename))\n",
    "        if overwrite.lower() != 'y':\n",
    "            return\n",
    "    print(\"Beginning file download...\")\n",
    "    r = requests.get(url)\n",
    "    if not r.ok:\n",
    "        print('Download failed')\n",
    "        return\n",
    "    with open(filepath, 'wb') as f:  \n",
    "        f.write(r.content)\n",
    "    print('File written to {}\\n'.format(filepath))\n",
    "    return\n",
    "    \n",
    "    \n",
    "def get_sales_data(path, sales_filename, license_filename):\n",
    "    \"\"\"\n",
    "    Gets links for most up-to-date dispensary sales and license information\n",
    "    from WSLCB and downloads datasets\n",
    "    Input:\n",
    "    Output: downloaded files to raw data directory\n",
    "    \"\"\"\n",
    "    # Get urls for most up-to-date sales and license data\n",
    "    url = 'https://lcb.wa.gov/records/frequently-requested-lists'\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = soup.find_all('a')\n",
    "        for link in links:\n",
    "            if 'Traceability' in link.text:\n",
    "                sales_url = link['href']\n",
    "                print(f'\\nLatest sales data found:\\n{sales_url}')\n",
    "                #filename = 'sales_data.xlsx'\n",
    "                download_dataset(sales_url, path, sales_filename)\n",
    "            elif 'Applicants' in link.text:\n",
    "                licenses_url = link['href']\n",
    "                print(f'\\nLatest license data found:\\n{licenses_url}')\n",
    "                #filename = 'license_data.xls'\n",
    "                download_dataset(licenses_url, path, license_filename)\n",
    "    else:\n",
    "        print('Failed to download sales data')\n",
    "\n",
    "    return\n",
    "\n",
    "    \n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that download data several\n",
    "    sources and saves those datasets to the data/raw directory.\n",
    "    \"\"\"\n",
    "    path = '../data/raw/'\n",
    "    \n",
    "    sales_filename = 'sales_data.xlsx'\n",
    "    license_filename = 'license_data.xls'\n",
    "    demo_filename = 'demographics.csv'\n",
    "    disp_filename = 'dispensary_data.json'\n",
    "    \n",
    "    get_sales_data(path, sales_filename, license_filename)\n",
    "    get_demo_data(path, license_filename, demo_filename)\n",
    "    get_leafly_disp_data(path, disp_filename)\n",
    "    \n",
    "    print('\\nData acquisition complete.\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latest license data found:\n",
      "https://lcb.wa.gov/sites/default/files/publications/Public_Records/2019/MarijuanaApplicants.xls\n",
      "license_data.xls already exists. Update? y/ny\n",
      "Beginning file download...\n",
      "File written to ../data/raw/license_data.xls\n",
      "\n",
      "Latest sales data found:\n",
      "https://lcb.wa.gov/sites/default/files/publications/Marijuana/sales_activity/2019-04-10-MJ-Sales-Activity-by-License-Number-Traceability-Contingency-Reporting.xlsx\n",
      "sales_data.xlsx already exists. Update? y/ny\n",
      "Beginning file download...\n",
      "File written to ../data/raw/sales_data.xlsx\n",
      "Demographics file already exists. Scrape data again? y/nn\n",
      "Initial dispensary list already exists. Scrape data again? y/nn\n",
      "Dispensaries data dict already exists. Scrape data again? y/nn\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub the Data\n",
    "\n",
    "*Look through the raw data files and see what you will need to do to them in order to have a workable data set. If your source data is already well-formatted, you may want to ask yourself why it hasn't already been analyzed and what other people may have overlooked when they were working on it. Are there other data sources that might give you more insights on some of the data you have here?*\n",
    "\n",
    "*The end goal of this step is to produce a [design matrix](https://en.wikipedia.org/wiki/Design_matrix), containing one column for every variable that you are modeling, including a column for the outputs, and one row for every observation in your data set. It needs to be in a format that won't cause any problems as you visualize and model your data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/features/build_features.py\n",
    "\n",
    "# imports\n",
    "def join_sales_data():\n",
    "    \"\"\"\n",
    "    Loads sales and license data files and joins them into one table\n",
    "    Also creates a column with zip code as a 5 digit string for later use\n",
    "    Input:\n",
    "    Output: returns merged dataframe\n",
    "    \"\"\"\n",
    "    path = '../data/raw/sales_data.xlsx'\n",
    "    disp_sales_data = pd.read_excel(path, sheet_name=0, header=3)\n",
    "    disp_sales_data.rename(columns={'License Number':'License #'}, inplace=True)\n",
    "    disp_sales_data.set_index(keys='License #', inplace=True)\n",
    "\n",
    "    path = '../data/raw/license_data.xls'\n",
    "    license_data = pd.read_excel(path, sheet_name=2, header=0, index_col=1)\n",
    "\n",
    "    sales_data = pd.merge(disp_sales_data, license_data, how='left', on='License #')\n",
    "    sales_data['zip_code'] = sales_data['ZipCode'].astype(str).str[:5]\n",
    "    \n",
    "    return sales_data\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from data/raw, \n",
    "    cleans them, and converts the data into a design matrix that is ready\n",
    "    for modeling.\n",
    "    \"\"\"\n",
    "    # clean_dataset_1('data/raw', filename)\n",
    "    # clean_dataset_2('data/raw', filename)\n",
    "    # save_cleaned_data_1('data/interim', filename)\n",
    "    # save_cleaned_data_2('data/interim', filename)\n",
    "    # build_features()\n",
    "    # save_features('data/processed')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before moving on to exploratory analysis, write down some notes about challenges encountered while working with this data that might be helpful for anyone else (including yourself) who may work through this later on.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data\n",
    "\n",
    "*Before you start exploring the data, write out your thought process about what you're looking for and what you expect to find. Take a minute to confirm that your plan actually makes sense.*\n",
    "\n",
    "*Calculate summary statistics and plot some charts to give you an idea what types of useful relationships might be in your dataset. Use these insights to go back and download additional data or engineer new features if necessary. Not now though... remember we're still just trying to finish the MVP!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/visualization/visualize.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from \n",
    "    data/processed, calculates descriptive statistics for the population,\n",
    "    and plots charts that visualize interesting relationships between \n",
    "    features.\n",
    "    \"\"\"\n",
    "    # data = load_features('data/processed')\n",
    "    # describe_features(data, 'reports/')\n",
    "    # generate_charts(data, 'reports/figures/')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What did you learn? What relationships do you think will be most helpful as you build your model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the Data\n",
    "\n",
    "*Describe the algorithm or algorithms that you plan to use to train with your data. How do these algorithms work? Why are they good choices for this data and problem space?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/models/train_model.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from \n",
    "    data/processed, calculates descriptive statistics for the population,\n",
    "    and plots charts that visualize interesting relationships between \n",
    "    features.\n",
    "    \"\"\"\n",
    "    # data = load_features('data/processed/')\n",
    "    # train, test = train_test_split(data)\n",
    "    # save_train_test(train, test, 'data/processed/')\n",
    "    # model = build_model()\n",
    "    # model.fit(train)\n",
    "    # save_model(model, 'models/')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile ../src/models/predict_model.py\n",
    "\n",
    "# imports\n",
    "# helper functions go here\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Executes a set of helper functions that read files from \n",
    "    data/processed, calculates descriptive statistics for the population,\n",
    "    and plots charts that visualize interesting relationships between\n",
    "    features.\n",
    "    \"\"\"\n",
    "    # test_X, test_y = load_test_data('data/processed')\n",
    "    # trained_model = load_model('models/')\n",
    "    # predictions = trained_model.predict(test_X)\n",
    "    # metrics = evaluate(test_y, predictions)\n",
    "    # save_metrics('reports/')\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write down any thoughts you may have about working with these algorithms on this data. What other ideas do you want to try out as you iterate on this pipeline?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret the Model\n",
    "\n",
    "_Write up the things you learned, and how well your model performed. Be sure address the model's strengths and weaknesses. What types of data does it handle well? What types of observations tend to give it a hard time? What future work would you or someone reading this might want to do, building on the lessons learned and tools developed in this project?_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project2)",
   "language": "python",
   "name": "project2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
